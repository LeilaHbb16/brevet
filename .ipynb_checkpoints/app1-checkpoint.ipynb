{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Donnons la liste de répertoire \n",
    "dossier_path = \"brevets_alternants\"\n",
    "categories = ['CHIMIE', 'GENIE ELECTRIQUE', 'GENIE MECANIQUE', 'INSTRUMENTS', 'AUTRES']\n",
    "\n",
    "# Initialiser une liste pour stocker les données de chaque brevet\n",
    "\n",
    "dfs = []\n",
    "# Parcourir chaque catégorie de brevets\n",
    "for category in categories:\n",
    "    data = []\n",
    "    category_path = os.path.join(dossier_path, category)\n",
    "    #print(category_path)\n",
    "    for file_name in os.listdir(category_path):\n",
    "        #print(file_name)\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(category_path, file_name)\n",
    "            #print(file_path)\n",
    "            #file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "            #print(file_path)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                file_content = json.load(f)\n",
    "            description = file_content.get('description', None)\n",
    "\n",
    "            #if description is not None:\n",
    "                # Nettoyer la description\n",
    "                #description_nettoyee = clean_text(description)\n",
    "\n",
    "                # Créer un DataFrame avec la description nettoyée et la catégorie\n",
    "            df = pd.DataFrame({'Description': description, 'Categorie': category})\n",
    "\n",
    "            dfs.append(df)  \n",
    "\n",
    "    # Créer un DataFrame à partir des données\n",
    "    df = pd.DataFrame(data)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfs[0][0]\n",
    "data = dfs[0]\n",
    "for i in range(1,len(dfs)) : \n",
    "    data = pd.concat([data, dfs[i]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquettes de cibles encodées avec LabelEncoder :\n",
      "[1 1 1 ... 0 0 0]\n",
      "\n",
      "Vecteurs one-hot correspondants :\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modzi\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "# Etape 1 : Encoder avec LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(data['Categorie'])\n",
    "\n",
    "print(\"Etiquettes de cibles encodées avec LabelEncoder :\")\n",
    "print(y_train_encoded)\n",
    "\n",
    "# Etape 2 : Conversion en vecteurs one-hot avec OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)  # sparse=False pour obtenir des tableaux Numpy au lieu de matrices creuses\n",
    "y_train_onehot = onehot_encoder.fit_transform(y_train_encoded.reshape(-1, 1))\n",
    "\n",
    "print(\"\\nVecteurs one-hot correspondants :\")\n",
    "print(y_train_onehot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CHIMIE', 'GENIE ELECTRIQUE', 'GENIE MECANIQUE', 'INSTRUMENTS',\n",
       "       'AUTRES'], dtype=object)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Categorie'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\modzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\modzi\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Prétraitement des données \n",
    "\n",
    "import re\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…',\n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─',\n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞',\n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_data(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "\n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "data['Description'] = data['Description'].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 4684\n",
      "Longest comment size: 704\n",
      "Average comment size: 84.4317880794702\n",
      "Stdev of comment size: 70.75714680479344\n",
      "Max comment size: 296\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = tokenizer = Tokenizer(num_words=None,lower=True,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',split=' ',char_level=False)\n",
    "\n",
    "tokenizer.fit_on_texts(data['Description'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "X = tokenizer.texts_to_sequences(data['Description'])\n",
    "\n",
    "vocab_size = len(word_index)\n",
    "print('Vocab size: {}'.format(vocab_size))\n",
    "longest = max(len(seq) for seq in X)\n",
    "print(\"Longest comment size: {}\".format(longest))\n",
    "average = np.mean([len(seq) for seq in X])\n",
    "print(\"Average comment size: {}\".format(average))\n",
    "stdev = np.std([len(seq) for seq in X])\n",
    "print(\"Stdev of comment size: {}\".format(stdev))\n",
    "max_len = int(average + stdev * 3)\n",
    "print('Max comment size: {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['Description'], y_train_onehot, test_size=0.3, stratify=data['Categorie'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1057, 296)\n",
      "x_test shape: (453, 296)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "processed_x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "processed_x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "processed_pre_x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "processed_pre_x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "print('x_train shape:', processed_x_train.shape)\n",
    "print('x_test shape:', processed_x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_dim = 50\n",
    "k = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        k += 1\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)       [(None, 296)]                0         []                            \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)       [(None, 296)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_18 (Embedding)    (None, 296, 50)              234250    ['input_19[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_19 (Embedding)    (None, 296, 50)              234250    ['input_20[0][0]']            \n",
      "                                                                                                  \n",
      " bidirectional_18 (Bidirect  (None, 296, 120)             53280     ['embedding_18[0][0]']        \n",
      " ional)                                                                                           \n",
      "                                                                                                  \n",
      " bidirectional_19 (Bidirect  (None, 296, 120)             53280     ['embedding_19[0][0]']        \n",
      " ional)                                                                                           \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)        (None, 296, 120)             0         ['bidirectional_18[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)        (None, 296, 120)             0         ['bidirectional_19[0][0]']    \n",
      "                                                                                                  \n",
      " attention_18 (Attention)    (None, 120)                  416       ['dropout_27[0][0]']          \n",
      "                                                                                                  \n",
      " attention_19 (Attention)    (None, 120)                  416       ['dropout_28[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate  (None, 240)                  0         ['attention_18[0][0]',        \n",
      " )                                                                   'attention_19[0][0]']        \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 50)                   12050     ['concatenate_9[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)        (None, 50)                   0         ['dense_18[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 50)                   200       ['dropout_29[0][0]']          \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_19 (Dense)            (None, 5)                    255       ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 588397 (2.24 MB)\n",
      "Trainable params: 588297 (2.24 MB)\n",
      "Non-trainable params: 100 (400.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, concatenate, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def get_model():\n",
    "    review_input = Input(shape=(max_len,), dtype='int32')\n",
    "    review_input_post = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "    x1 = Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True)(review_input)\n",
    "    x1 = Bidirectional(LSTM(60, return_sequences=True))(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = Attention(max_len)(x1)\n",
    "\n",
    "    x2 = Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True)(review_input_post)\n",
    "    x2 = Bidirectional(LSTM(60, return_sequences=True))(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = Attention(max_len)(x2)\n",
    "\n",
    "    x = concatenate([x1, x2])\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    x= Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    preds = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=[review_input, review_input_post], outputs=preds)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "34/34 [==============================] - 16s 267ms/step - loss: 1.3748 - accuracy: 0.4447 - val_loss: 1.4728 - val_accuracy: 0.4547\n",
      "Epoch 2/15\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 1.0267 - accuracy: 0.5960 - val_loss: 1.3375 - val_accuracy: 0.5629\n",
      "Epoch 3/15\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.8785 - accuracy: 0.6528 - val_loss: 1.2160 - val_accuracy: 0.6049\n",
      "Epoch 4/15\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.8195 - accuracy: 0.6613 - val_loss: 1.1364 - val_accuracy: 0.6071\n",
      "Epoch 5/15\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.7306 - accuracy: 0.6755 - val_loss: 1.0207 - val_accuracy: 0.6512\n",
      "Epoch 6/15\n",
      "34/34 [==============================] - 9s 257ms/step - loss: 0.6709 - accuracy: 0.6963 - val_loss: 0.9024 - val_accuracy: 0.6291\n",
      "Epoch 7/15\n",
      "34/34 [==============================] - 9s 257ms/step - loss: 0.6414 - accuracy: 0.7058 - val_loss: 0.8142 - val_accuracy: 0.6578\n",
      "Epoch 8/15\n",
      "34/34 [==============================] - 9s 264ms/step - loss: 0.6371 - accuracy: 0.7143 - val_loss: 0.7599 - val_accuracy: 0.6711\n",
      "Epoch 9/15\n",
      "34/34 [==============================] - 9s 258ms/step - loss: 0.5734 - accuracy: 0.7200 - val_loss: 0.7514 - val_accuracy: 0.6358\n",
      "Epoch 10/15\n",
      "34/34 [==============================] - 9s 258ms/step - loss: 0.5577 - accuracy: 0.7190 - val_loss: 0.7304 - val_accuracy: 0.6181\n",
      "Epoch 11/15\n",
      "34/34 [==============================] - 9s 266ms/step - loss: 0.5368 - accuracy: 0.7342 - val_loss: 0.6742 - val_accuracy: 0.6446\n",
      "Epoch 12/15\n",
      "34/34 [==============================] - 9s 258ms/step - loss: 0.5233 - accuracy: 0.7474 - val_loss: 0.6615 - val_accuracy: 0.6159\n",
      "Epoch 13/15\n",
      "34/34 [==============================] - 9s 257ms/step - loss: 0.5288 - accuracy: 0.7323 - val_loss: 0.7819 - val_accuracy: 0.6137\n",
      "Epoch 14/15\n",
      "34/34 [==============================] - 9s 262ms/step - loss: 0.5420 - accuracy: 0.7370 - val_loss: 0.7100 - val_accuracy: 0.6733\n",
      "Epoch 15/15\n",
      "34/34 [==============================] - 9s 258ms/step - loss: 0.5083 - accuracy: 0.7342 - val_loss: 0.7442 - val_accuracy: 0.6380\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([processed_x_train,processed_pre_x_train],y_train, validation_data=([processed_x_test,processed_pre_x_test],y_test), epochs=15,batch_size=32,callbacks=[early_stopping_monitor],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 71ms/step - loss: 0.7442 - accuracy: 0.6380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7442281246185303, 0.6379690766334534]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([processed_x_test,processed_pre_x_test],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(processed_x_train, y_train)\n",
    "prediction = clf.predict(processed_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5146641438032167\n",
      "0.3090507726269316\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(processed_x_train, y_train))\n",
    "print(clf.score(processed_x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
